# Project Overview:
Our main data source is Reddit. We used a collection of the top 50 posts with most user traffic from the subreddit ‘nosleep’ - a discussion board where authors share original horror stories. We also used fragments from different newspaper publications specifically for sentiment analysis.

We hoped to learn:
*  What devices (i.e words) are effective in creating successful horror stories?
* Which words are frequently used across all horror stories
* How can NLP be used to estimate sentiment in text?

We analyzed the data through the following techniques: 

* Pulled stories into text files using the Reddit API and I/O programming. 
* Cleaned the text using simple functions. 
* Counted word and letter frequencies using dictionaries, lists, and tuples. 
* Compared the similarity of “hot” and “top of all time” posts by word frequency. 
* Conducted sentiment analysis using NLP

# Implementation:
We chose to analyze ‘nosleep’ posts in two main categories, the hot (essentially trending) and top (highest upvoted of all time) sections. This decision was made so that we would be working with only the most popular stories, which is helpful for understanding the basis of a good horror story. When we harvested these stories, we first stored them as .txt files before performing analysis on them. There were also a few libraries that we had to import in order to properly execute this analysis. The main one we used is called praw, which is an API that can gather data from Reddit. Additionally, we imported the library nltk to perform sentiment analysis.

For frequency analysis, we started by creating functions to filter numbers, special characters, and filler words from the text in the posts. We then created a function to assign every unique word in the text to a histogram that shows its frequency in the text. Furthermore, we added additional functions to sort and filter the words in the histogram by frequency so that the user can get the most relevant output. Based on the code for this program, we created another program that outputs the frequency distribution of ‘scary words’. To accomplish this, we added an additional filtering function at the beginning to filter only the ‘scary’ words in addition to the ones that are fillers and the ones that have special characters. Lastly, we conducted the same functions and process but to analyze the frequency of letters instead of words. In this case, we wrote the histogram function so that it adds letter by letter to the histogram instead of word by word.

For the text similarity analysis we compared the currently trending “hot” posts to the top five posts of all time in “nosleep”. We compiled the trending posts into one text document named textsimilarity.txt and created a dictionary of words and frequencies out of it. Then we created histograms of the top five posts in “nosleep” history and created dictionaries of “prominent words” that were used in both the hot posts and any one of the top posts. Finally we printed the dictionaries of common words along with the number of words in common for each of the top five posts of all time.
	
For sentiment analysis, we started by writing a function to isolate individual posts into text files so that we can analyze the sentiment of each post separately. Once we had a text file with each separate post, we wrote a function that conducts the polarity analysis from a file, returning a dictionary with polarity scores.

In general, our project is focussed around analyzing common traits, words, and structure in scary stories. We had to sacrifice some preciseness in the functionality of the code in order to broadly analyze posts. We felt that targeting a larger sample size of posts (5-50) was more important than deeply analyzing specific posts since open ended stories are likely to be dissimilar from each other. Therefore, we determined that larger sample sizes would likely yield more accurate and useful results from the project. 
# Results:
## Word Frequency Analysis:
The word frequency analysis yielded interesting results that provide insight into what grammatical devices are effective in creating horror stories. The frequency of the top ‘scary’ words are shown in the first bar graph below. 3 of the 5 most common ‘scary’ words used in horror stories are ‘screaming’ and ‘shaking’. Other ‘scary’ words most commonly used included ‘terror’, ‘scared’ and ‘crying’. Interestingly, many of these words are verbs describing emotions which result from horror, rather than horrific objects themselves. Some of the most common ‘non-scary’ words were ‘something’, ‘eyes’, ‘time’, ‘looked’, and ‘little’. The frequency of these regular words are shown in the second bar graph below. Curiously, the percentage of total words used that are ‘scary’ resulted to be only 0.85%. This suggests that, although scary words are important in horror stories, the horror is set by how these words are used in combination with regular ones. Lastly, our letter frequency analysis revealed that the top 5 letters used are ‘e’, ‘t’, ‘a’, ‘o’,and ‘i’. The frequency of these letters can be found in the third bar graph below. These results suggest that vowels were used very frequently compared to other letters, which is common with most texts in english.

![image](https://user-images.githubusercontent.com/51338302/111524919-63a87b80-8733-11eb-8aef-1d1dbfdaaf41.png)

![image](https://user-images.githubusercontent.com/51338302/111525244-bf730480-8733-11eb-940a-de77a18e417d.png)

![image](https://user-images.githubusercontent.com/51338302/111525324-db76a600-8733-11eb-90f3-4cd45a5eeddf.png)


## Text Similarity Analysis:
We used text similarity analysis to compare the prominence of any given word in both the most recent “hot” posts on nosleep and the top posts of all time. This was conducted by creating dictionaries with words as keys and frequencies as values. The results yielded by this analysis revealed commonly used words across popular horror stories, which can be used to improve horror writing ability. For the top five posts of all time, the average number of predominant words in common with the currently trending posts was 23.2, with the longer stories on average yielding higher numbers of words in common (due to the higher word totals). One interesting take away from the words used is that they were all accessible. They all are easy to understand at any reading level, are generally short in length, and are commonly used. This makes sense since they had to be used in multiple stories to register with the program. Many of the words were also verbs that are often used in cases of suspense. Two examples are “looked” which occurred in 3 of the top 5 posts and “know” which appeared 3 times total. “Like” was unsurprisingly used in every story to create connections from fictional events to real life ones that people can better interpret and react to. One final word that we found interesting was “time” which was used in 3 of the top 5 posts to create a sense of urgency and suspense. Overall, we determined that popular horror stories must be accessible with a low reading level and word length while building suspense through traits people associate with (e.g. using time to create urgency) to properly scare a large number of people.

## Sentiment Analysis:
We applied sentiment analysis to newspaper publications as it was not as relevant for the horror stories. Good newspapers are usually applauded for their objective fact reporting. This includes being mindful of sensational headlines and avoiding subjective language, for the most part. Using the natural language processing toolkit, our group took a look at fragments from different newspaper publications on two events: 1) the U.S. rejoining the Paris Climate Accord, and 2) Trump being convicted for the insurrection at the Capitol in January. Our results were the following:  according to the natural language processing toolkit, the tone of Fox’s article about rejoining the PCA was indicated to be neutral (negative leaning), whereas the tones of WSJ and Huffington Post were indicated to be positive. Additionally, the tone of Fox’s article about Trump being convicted for the insurrection was indicated to be neutral, whereas the tones of WSJ and Huffington Post were indicated to be positive towards Trump’s conviction. Our biggest takeaway is that though these tones and this ‘sprinkle’ of subjectivity is subtle, and therefore we don’t always register it to be less than factual reporting, it is important to keep in mind that it’s there, because it can be picked up by our algorithm. So being a critical reader who questions news in a healthy way, is a best practice, as well as diversifying your news sources to include multiple angles.  
# Reflection:
Our process for working on this project was the following. We had an initial meeting during which we decided what data source to use and familiarized ourselves with how to access Reddit through the API. Once we all knew how to access Reddit's text, we divided the work by assigning one type of analysis to each team member. We essentially checked-in every other day for anywhere between 30 and 90 mins to work through blockers, bring up other scoping opportunities, and do code reviews. At the same time, we wanted to each learn something different by ourselves before sharing those learnings with the rest of the group. Lastly, we met to bring our analysis together and reflect on our findings.
In retrospect, many things went well. We are happy to have started early since the project requires quite some collaboration. Having frequent, shorter check-ins instead of planning a one-time 4 to 6-hour meeting worked well for us. This process was successful because we would have had to pick one analysis focus instead of playing with all three simultaneously if we had done everything in a group meeting. Additionally, Slack was a great vehicle for getting a quick, informal review of ‘broken’ code. On the other hand, something we could have improved on is visibility into each other’s work. Although our check-ins were efficient, each one of us got significantly more exposure than the rest to one specific type of analysis. With more time, we could have potentially worked on all of the analysis together, although that change would sacrifice some efficiency.
